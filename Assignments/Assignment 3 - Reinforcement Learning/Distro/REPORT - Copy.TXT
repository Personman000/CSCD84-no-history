CSC D84 - Artificial Intelligence

Assignment 3 - Reinforcement Learning - Q Learning

This assignment is worth:

10 AIUs (Artificial Intelligence Units)
toward the assignment component of your final
mark.

________________________________________________

Student Name 1 (last, first): Ali, Saad

Student Name 2 (last, first):

Student number 1: 1002519269

Student number 2:

UTORid 1: alisaad2

UTORid 2:

READ THIS AND SIGN YOUR NAME AT THE END:

 I certify that I have read the UTSC code on academic
honesty and plaguarism. All work submitted as part
of this assignment is my own.

	Signed: _Saad Syed Ali__	_(student 2 name here)__


(-5 marks for failing to provide the identifying
 information requested above)
________________________________________________

Answer the following questions. Be concise and clear
but explain carefully when needed.

1 .- (1 mark) Explain your reward function. What game elements
      are used, and why you think this is a good reward function
      for this problem.
      
      My reward function is very simple. Maximum reward for if the mouse is on the same node as the cheese. Maximum penalty if the mouse is on the same node as the cat. The intermediate nodes in the QTable will be filled in naturally with the Q learning procedure. I found that adding more complicated calculations generally decreased the win rate. This simple reward function makes sure that there is no confusing or unexpected behavior.

2 .- These are multiple experiments (once you are sure your 
     QLearning code is working!) 

     * IMPORTANT! * For each training run, SAVE the Qtable
     you will re-use the Qtables later.

     (1 mark) # Experiment 1, 10000 training trials, 20 rounds, random seed 1522,
     # 8x8 grid, cat-smartness .9

# Note: I am using 1000000 trials for Experiment 1, as stated in https://piazza.com/class/kjiq7ybeoap1eg?cid=246

     Initial mouse winning rate (first rate obtained when training starts):0.064511

     Train your mouse, and once trained, run the evaluation and  
     record the mouse winning rate:0.337205

     (1 mark) # Experiment 2, 1000000 training trials, 50 rounds, random seed 1522,
     # 8x8 grid, cat-smartness .9

     Initial mouse winning rate (first rate obtained when training starts):0.064076

     Train your mouse, and once trained, run the evaluation and  
     record the mouse winning rate:0.328943

     (1 mark) Would the mouse keep improving and become invincible if
     you ran, say, 100,000,000 training rounds per batch?
     
     I would say no. As I tried many different reward functions, I noticed for almost all of them that there was a maximum limit that the win rate would reach. In these cases, there was little or no change between running something like 5000000 rounds versus 10000000 rounds. This leads me to believe that there is a maximum limit of win rate achievable by any reward function or Q Learning algorithm, and that a 100% win rate is not achievable, or at least very improbable.

4 .- Using the QTable saved from Experiment 2 (NO re-training!)

     (1 mark) # Experiment 3 - Run the evaluation on the 8x8 grid, cat-smartness=.9,
     # but using the random seed 4289 for the game.
	
     Mouse Winning Rate:0.194939

     (2 mark) # Experiment 4 - Run the evaluation on the 8x8 grid, cat-smartness=.9,
     # but using the random seed 31415 for the game.
	
     Mouse Winning Rate:0.197695

     Average rate for Experiement 3 and Experiment 4:0.196317

     (1 mark) Compare with the rate obtained in experiment 2. What is happening here?
     
     Our win rate is decreasing because our mouse was initially trained in the maze generated by seed 1522. The behavior learned in 1522 may not be as optimal in other mazes generated with other seeds, and as such, the mouse is losing more often in the new seeds than its original seed.

5 .- Understanding the effects of state space on learning ability:

     (1 mark) # Experiment 5 - Train your mouse on the 16x16 grid, using random seed
     # 1522, 1000000 trials, and 50 training rounds. Cat-smartness=.9

     Initial mouse winning rate (first rate obtained when training starts):0.029474

     Mouse Winning Rate (from evaluation after training):0.120698

     (1 mark) Compare the final winning rate with that from Experiment 2, what can
     you say about the difference in winning rates?
     
     The difference in win rates was much higher in Experiment 2 than in Experiment 5. This is likely because of the increased state-space of the 16x16 maze makes it a lot harder to learn all the required behavior for an optimal policy. Maybe this could be countered by increased the number of training rounds to something much higher, maybe even beyond the limit that the current driver code allows.

6 .- (2 marks) Is standard Q-Learning a rasonable strategy for environments
     that change constantly? discuss based on the above
     
     No. Q-Learning works well for environments that do not change. As soon as a change is introduced, the algorithm is using data trained for environment 1 in the new environment 2, expecting everything in the new environment to be the exact same. This means it cannot account for these new changes in its policy, and such will fail more often in these new, constantly changing environments.

7 .- (3 marks) Explain your feature set for feature-based Q Learning, and
               explain why and how each feature is expected to contribute
	       to helping your mouse win
	       
     My feature set is also quite simple. Feature0 is the euclidean distance to the nearest cheese. Feature1 is the euclidean distance to the nearest cat. I expect the agent to quickly learn that maximising Weight0 and minimising Weight1 to be optimal, as those are the situations which cause win states to happen.

8 .- Carry out the following experiments:

     (1 mark) # Experiment 6: Train your mouse using feature-based Q-Learning, on
     # the 16x16 grid, one cat, and one cheese, cat-smartness=.9,  
     # random seed = 1522, and use 1000000 trials and 50 rounds. 

     Initial mouse winning rate (first rate obtained when training starts):0.029802
     
     Mouse Winning Rate (from evaluation after training):0.280422

     Compare this rate against the rate obtained in Experiment #5, what
     are your conclusions regarding the utility of feature-based
     Q-Learning?
     
     With almost a win rate that more than two times better, feature-based Q-Learning easily works much better when the state space is larger.

     (1 mark) # Experiment 7, using the saved weights from Experiment 6 (no
     # re-training) run the evaluation using random seed 4289, same
     # 1 cheese, 1 cat, and cat-smartness=.9, 16x16 grid

     Mouse Winning Rate (from evaluation after training):0.283587

     (1 mark) # Experiment 8, using the saved weights from Experiment 6 (no
     # re-training) run the evaluation using random seed 31415, same
     # 1 cheese, 1 cat, and cat-smartness=.9, 16x16 grid

     Mouse Winning Rate (from evaluation after training):0.286601

     (1 mark) Comparing the winning rates from Experiments #7 and #8 against
     those from Experiment #6, what can you conclude about feature-
     based Q-Learning in terms of its ability to deal with changes 
     to the environment?
     
     Feature Q-Learning is very effective in dealing with changing environments! For my experiments, the Feature Q-Learning algorithm not only matched, but did better in its new environment than it did in the original one it was trained in. While this is likely not always the case, just a pleasant coincidence here, the fact that it can happen at all, and that the new win rate can match up so regularly with the original training win rate just goes to show the effectiveness of Feature Q-Learning in this situation.
     
9 .- Carry out the following experiments:

     (1 mark) # Experiment 9: Train your mouse using feature-based Q-Learning, on
     # the 16x16 grid, 2 cats, and 3 cheeses, cat-smartness=.9,  
     # random seed = 1522, and use 1000000 trials and 50 rounds. 

     Initial mouse winning rate (first rate obtained when training starts):0.082149
     
     Mouse Winning Rate (from evaluation after training):0.446758
  
     (1 mark) # Experiment 9: Using the trained weights from Experiment 9, (no
     # re-training), run the evaluation on the 16x16 grid, 2 cats, 
     # 3 cheeses, cat-smartness=.9, and random seed 4289
     
     Mouse Winning Rate (from evaluation after training):0.448152

     (1 mark) # Experiment 9: Using the trained weights from Experiment 9, (no
     # re-training), run the evaluation on the 32x32 grid, 2 cats, 
     # 3 cheeses, cat-smartness=.9, and random seed 4289
     
     Mouse Winning Rate (from evaluation after training):0.310012

     (2 marks) Based on the last 3 experiments, what can you conclude regarding
     the usefulness and applicability of standard Q-Learning vs. 
     feature-based Q-learning?
     
     While standard Q-Learning can be useful in static environments, feature-based Q-Learning can be significantly more effective and efficient. Since real world applications are often filled with uncertanties and constantly changing environments, the benefits of feature-based Q-Learning seem to beat standard Q-Learning in a lot of ways. Just off the top of my head, it is easy to think of many situations where feature-based Q-Learning can be much more useful in real-life applications because of its effectiveness in multiple situations (e.g. drone pathfinding) and efficiency in processing (e.g. gps mapping).

10 . - (2 marks) We obviously can not train an expensive robot by trial and error,
      How would you implement feature-based Q-learning for a mobile bot so you
      can take advantage of reinforcement learning but not spend millions on
      broken robots that keep falling down the stairs?
      
      I would create a virtual environment simulation that the feature-based Q-Learning agent can read and navigate through in order to learn from. This simulation would be made to be as close as possible to whatever real-life situations the robot would have to face would be, and would be as varying as possible within those parameters for effective learning. For example, for a robot that must be able to find efficient paths through the uneven and treacherous terrain of mars, I would create a virtual simulation of mars using either randomly generated heights and terrain features that are similar to the mars surface, or randomly place the robot on a single point on an encoded terrain-map of mars itself as part of its learning process. This way, the robot's software would learn the ropes through its virtual simulation environment before the hardware has to take any hits while off-planet.
_____________________________________________________

Mark with an 'x' where appropriate. If something is only
working partially, briefly describe what works, what
doesn't work, or what problems exist.
	
			Complete/Working	Partial		Not done

QLearn 			X
 update

Reward				X
 function

Decide				X
 action

featureEval			X

evaluateQsa			X

maxQsa_prime			X

Qlearn_features		X

decideAction_features		X

_____________________________________________________

Marking:

(10 marks) Implemented QLearn update.

(5 marks) Implemented a reasonable reward function

(5 marks)  Implemented the function that chooses the
           optimal action based on Qtable and
           maze

(15 marks) Implemented a non-trivial, good feature set
	   and evaluation function

(10 marks) Implemented a working feature-based Q-learning
	   algorithm

(20 marks) Competitive mouse performance

(15 marks) Answers in this report file

(- marks)  Penalty marks

Total for A3:       / out of 80


